seed: 1986
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]

data_folder: ./processed_data
output_folder: !ref ./results/la_fastaudiogauss/<seed>
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

# Path where data manifest files will be stored
# The data manifest files are created by the data preparation script.
train_annotation: !ref <data_folder>/la_cm_merge.json
dev_annotation: !ref <data_folder>/la_cm_dev.json
eval_annotation: !ref <data_folder>/la_cm_eval_2021.json


# The train logger writes training statistics to a file, as well as stdout.
train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

error_stats: !name:speechbrain.utils.metric_stats.MetricStats
    metric: !name:speechbrain.nnet.losses.classification_error
        reduction: batch

ckpt_interval_minutes: 15 # save checkpoint every N min

# Feature parameters
n_mels: 40
embedding_incha: 40
embedding_features: [fastaudiogauss]


# Training Parameters
sample_rate: 16000
number_of_epochs: 100
batch_size: 4
lr_start: 0.0001
lr_final: 0.00001

n_classes: 1 # In this case, we have spoof and bonafide
emb_dim: 256 # dimensionality of the embeddings
shuffle: True

dataloader_options:
    batch_size: !ref <batch_size>



#########-----------------Modules---------------------##########
#########-----------------Start-----------------------##########

# Added noise and  reverberation come from OpenRIR dataset, automatically
# downloaded and prepared with this Environmental Corruption class.
env_corrupt: !new:speechbrain.lobes.augment.EnvCorrupt
    openrir_folder: !ref <data_folder>
    babble_prob: 0.0
    reverb_prob: 0.0
    noise_prob: 1.0
    noise_snr_low: 0
    noise_snr_high: 15

# Adds speech change + time and frequency dropouts (time-domain implementation)
# # A small speed change help to improve the performance of speaker-id as well.
augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment
    sample_rate: !ref <sample_rate>
    speeds: [95, 100, 105]

# Feature extraction
# ----- Option starts -----


# fbanks: !new:speechbrain.lobes.features.Fbank
#     n_mels: !ref <n_mels>
#     sample_rate: 16000
#     n_fft: 400
#     # requires_grad: True
#     # filter_shape: 'gaussian'

fastaudiogauss: !new:models.custom_model.FastAudio
    n_mels: !ref <n_mels>
    sample_rate: 16000
    n_fft: 400
    requires_grad: True
    filter_shape: 'gaussian'

fastaudiotri: !new:models.custom_model.FastAudio
    n_mels: !ref <n_mels>
    sample_rate: 16000
    n_fft: 400
    requires_grad: True
    filter_shape: 'triangular'

fastaudiosorted: !new:models.custom_model.FastAudio
    n_mels: !ref <n_mels>
    sample_rate: 16000
    n_fft: 400
    requires_grad: True
    filter_shape: 'triangular'
    sort: True


#trainable_fbanks: !new:nnAudio.Spectrogram.MelSpectrogram
#    sr: 16000
#    fmax: 8000
#    hop_length: 160
#    htk: True
#    trainable_mel: True
#    n_mels: !ref <n_mels>
#    n_fft: 400
#    hop_length: 32 #512/(16000/1000)

#pcen: !new:pcen.StreamingPCENTransform
#    n_mels: !ref <n_mels>
#    n_fft: 512
#    trainable: True

# leaf: !new:leaf_audio_pytorch.frontend.Leaf
# #    preemp: False
# #    compression_fn: False
#    learn_pooling: True
#    learn_filters: True
#    n_filters: !ref <n_mels>

# tdfbanks: !new:models.custom_model.TDFbanks
#    nfilters: !ref <n_mels>
#    samplerate: 16000
#    nfft: 512
#    mode: 'Learn-filterbank'


#mfcc: !new:speechbrain.lobes.features.MFCC
#    n_mels: !ref <n_mels>
#    sample_rate: 16000
#

#mag: !new:models.custom_model.mag
#    sample_rate: 16000

#insf: !new:models.custom_model.IF


# cqt: !new:nnAudio.Spectrogram.CQT
#    sr: 16000
#    hop_length: 256
#    trainable: False
#    n_bins: !ref <n_mels>

# ------ Option ends --------

# Mean and std normalization of the input features
mean_var_norm: !new:speechbrain.processing.features.InputNormalization
    norm_type: sentence
    std_norm: False

# embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector
#    in_channels: !ref <embedding_incha>
#    activation: !name:torch.nn.LeakyReLU
#    tdnn_blocks: 5
#    tdnn_channels: [512, 512, 512, 1024, 1500]
#    tdnn_kernel_sizes: [5, 3, 3, 1, 1]
#    tdnn_dilations: [1, 2, 3, 1, 1]
#    lin_neurons: !ref <emb_dim>

embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN
    input_size: !ref <embedding_incha>
    activation: !name:torch.nn.LeakyReLU
    channels: [512, 512, 512, 512, 1536]
    kernel_sizes: [5, 3, 3, 3, 1]
    dilations: [1, 2, 3, 4, 1]
    attention_channels: 128
    res2net_scale: 8
    se_channels: 128
    lin_neurons: !ref <emb_dim>

classifier: !new:speechbrain.lobes.models.Xvector.Discriminator
    input_shape: [null, null, !ref <emb_dim>]
    activation: !name:torch.nn.LeakyReLU
    lin_blocks: 1
    lin_neurons: 512
    out_neurons: !ref <n_classes>

compute_cost: !new:speechbrain.nnet.losses.LogSoftmaxWrapper
    loss_fn: !new:speechbrain.nnet.losses.AdditiveAngularMargin
        margin: 0.2
        scale: 30

# The first object passed to the Brain class is this "Epoch Counter"
# which is saved by the Checkpointer so that training can be resumed
# if it gets interrupted at any point.
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

# Objects in "modules" dict will have their parameters moved to the correct
# device, as well as having train()/eval() called on them by the Brain class.
modules:
#    mfcc: !ref <mfcc>
#    cqt: !ref <cqt>
#    mag: !ref <mag>
    # fbanks: !ref <fbanks>
    fastaudiogauss: !ref <fastaudiogauss>
#    pcen: !ref <pcen>
    # leaf: !ref <leaf>
#    mcqt: !ref <mcqt>
#    sincnet: !ref <sincnet>
#    insf: !ref <insf>
#    trainable_fbanks: !ref <trainable_fbanks>
    # tdfbanks: !ref <tdfbanks>

    env_corrupt: !ref <env_corrupt>
    augmentation: !ref <augmentation>
    embedding_model: !ref <embedding_model>
    classifier: !ref <classifier>
    mean_var_norm: !ref <mean_var_norm>


label_encoder: !new:speechbrain.dataio.encoder.CategoricalEncoder
#########-----------------Modules---------------------##########
#########-----------------End-----------------------##########


#########-----------------Trainer---------------------##########
#########-----------------Start-----------------------##########


# This optimizer will be constructed by the Brain class after all parameters
# are moved to the correct device. Then it will be added to the checkpointer.
opt_class: !name:torch.optim.Adam
    lr: !ref <lr_start>
    weight_decay: 0.000002

# This function manages learning rate annealing over the epochs.
# We here use the simple lr annealing method that linearly decreases
# the lr from the initial value to the final one.
lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler
    initial_value: !ref <lr_start>
    final_value: !ref <lr_final>
    epoch_count: !ref <number_of_epochs>

lr_scheduler: !new:speechbrain.nnet.schedulers.ReduceLROnPlateau
    factor: 0.8
    patience: 2
    dont_halve_until_epoch: 5

# This object is used for saving the state of training both so that it
# can be resumed if it gets interrupted, and also so that the best checkpoint
# can be later loaded for evaluation or inference.
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        # leaf: !ref <leaf>
        # tdfbanks: !ref <tdfbanks>
        # fbanks: !ref <fbanks>
#        trainable_fbanks: !ref <trainable_fbanks>
        fastaudiogauss: !ref <fastaudiogauss>
        embedding_model: !ref <embedding_model>
        classifier: !ref <classifier>
        normalizer: !ref <mean_var_norm>
        counter: !ref <epoch_counter>
